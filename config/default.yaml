# Environment
task: hopper
behavior: medium-replay-v2
val_behavior: expert-v2
discount: 0.99
observation_shape: [17,]
action_shape: [6,]
device: 'cuda:0'

# Trainer
batch_size: 64
epochs: 100


# Model
world_model:
  cls_name: mlp_dwm
  optim_cls: torch.optim.AdamW
  optim_kwargs: 
    weight_decay: 0.000001
  enc_arch: [1024, 1024, 512, 256]
  recon_arch: [256]   # if empty, one Linear Layer is applied
  reward_arch: [256]  # if empty, one Linear Layer is applied
  done_arch: [256]    # if empty, one Linear Layer is applied



# 
dyn_encoder:
  dyn_dim: 32  # final output size
  inp_size: 26
  learnable_init_state: False
  
  optim_cls: torch.optim.AdamW
  optim_kwargs:
    weight_decay: 0.000001
  arch: [256, 256, 256]
  cls_name: rnn_de
  use_trajectory: False
  concat: "OARN"  